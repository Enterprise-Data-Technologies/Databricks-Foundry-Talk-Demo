{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "markdown-0",
   "metadata": {},
   "source": [
    "# Foundry to Databricks Bronze Layer Pipeline\n",
    "\n",
    "This notebook demonstrates a data ingestion pipeline that extracts data from Palantir Foundry via JDBC and loads it into a Databricks bronze layer following the medallion architecture pattern.\n",
    "\n",
    "## Architecture Overview\n",
    "- **Bronze Layer**: Raw data ingestion with minimal transformation\n",
    "- **String Casting**: Initial data is cast to strings to handle schema variations\n",
    "- **Schema Enforcement**: Data is then cast back to proper types based on target schema\n",
    "- **Incremental Loading**: Uses append mode with merge schema for flexibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PySpark SQL functions for data transformations\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown-1",
   "metadata": {},
   "source": [
    "## JDBC Connection Helper Function\n",
    "\n",
    "This function creates a reusable JDBC reader for Palantir Foundry data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_foundry_jdbc(table, schema=None):\n",
    "    \"\"\"\n",
    "    Read data from Palantir Foundry using JDBC connection.\n",
    "    \n",
    "    Args:\n",
    "        table (str): The fully qualified table name or dataset resource identifier\n",
    "        schema (StructType, optional): PySpark schema to apply to the data\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: PySpark DataFrame containing the requested data\n",
    "    \"\"\"\n",
    "    # JDBC connection string for Foundry SQL interface\n",
    "    jdbc_url = \"jdbc:foundrysql://your-instance.palantirfoundry.com\"\n",
    "    \n",
    "    # Connection properties with secure credential management\n",
    "    properties = {\n",
    "        # Retrieve credentials from Databricks secrets for security\n",
    "        \"password\": dbutils.secrets.get(scope=\"your_scope\", key=\"your_token\"),\n",
    "        \"driver\": \"com.palantir.foundry.sql.jdbc.FoundryJdbcDriver\",\n",
    "    }\n",
    "\n",
    "    # Configure the JDBC reader with connection details\n",
    "    reader = (\n",
    "        spark.read.format(\"jdbc\")\n",
    "        .option(\"url\", jdbc_url)\n",
    "        .option(\"dbtable\", table)\n",
    "        .option(\"password\", properties[\"password\"])\n",
    "        .option(\"driver\", properties[\"driver\"])\n",
    "        .option(\"numPartitions\", 8)  # Parallelize read across 8 partitions for performance\n",
    "    )\n",
    "    \n",
    "    # Optionally apply a predefined schema\n",
    "    if schema is not None:\n",
    "        reader = reader.schema(schema)\n",
    "    \n",
    "    return reader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown-2",
   "metadata": {},
   "source": [
    "## Main Pipeline Function: Create Bronze Staging Layer\n",
    "\n",
    "This function orchestrates the ETL process:\n",
    "1. Reads a configuration table that lists all tables to export\n",
    "2. Iterates through each table\n",
    "3. Performs schema casting and validation\n",
    "4. Writes to bronze layer with audit timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "def create_staging_layer():\n",
    "    \"\"\"\n",
    "    Main ETL function to ingest data from Foundry into Databricks bronze layer.\n",
    "    \n",
    "    Process:\n",
    "    1. Read configuration table listing all source tables\n",
    "    2. For each table:\n",
    "       - Extract data from Foundry\n",
    "       - Cast to strings for safe initial handling\n",
    "       - Create temporary views for schema management\n",
    "       - Cast to proper data types\n",
    "       - Write to bronze layer with load timestamp\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Read the export tables configuration dataset\n",
    "    # This dataset contains SCHEMA and TABLE_NAME columns that define what to export\n",
    "    export_tables = read_foundry_jdbc(\n",
    "        '\"ri.foundry.main.dataset.YOUR-DATASET-ID-HERE\"'\n",
    "    )\n",
    "    \n",
    "    # Step 2: Create a dictionary mapping schema names to table names\n",
    "    # Format: {schema_name: \"table_name\"}\n",
    "    export_tables_dict = dict(\n",
    "        export_tables.select(\"SCHEMA\", \"TABLE_NAME\")\n",
    "        .rdd.map(lambda row: (row[\"SCHEMA\"], f'\"{row[\"TABLE_NAME\"]}\"'))\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    # Step 3: Iterate through each table to process\n",
    "    for key in export_tables_dict:\n",
    "        # Read the source data from Foundry\n",
    "        df = read_foundry_jdbc(export_tables_dict[key])\n",
    "        \n",
    "        # Capture the original schema before casting\n",
    "        schema = df.schema\n",
    "        \n",
    "        # Step 4: Cast all columns to strings initially\n",
    "        # This provides a safety layer to handle any data type inconsistencies\n",
    "        df_str = df.select([df[c].cast(\"string\").alias(c) for c in df.columns])\n",
    "\n",
    "        try:\n",
    "            # Step 5: Create temporary views for schema management\n",
    "            schema_view = f\"tmp_schema_{key}\"\n",
    "            raw_view = f\"tmp_raw_{key}\"\n",
    "            \n",
    "            # Create an empty DataFrame with original schema as reference\n",
    "            spark.createDataFrame([], schema).createOrReplaceTempView(schema_view)\n",
    "            \n",
    "            # Register the string-casted data as a temp view\n",
    "            df_str.createOrReplaceTempView(raw_view)\n",
    "\n",
    "            # Step 6: Extract the target schema from the schema view\n",
    "            target_schema = spark.table(schema_view).schema\n",
    "\n",
    "            # Step 7: Cast raw data back to proper types based on target schema\n",
    "            raw_df = spark.table(raw_view)\n",
    "            \n",
    "            # Build list of columns with proper type casting\n",
    "            casted_cols = [\n",
    "                F.col(f.name).cast(f.dataType).alias(f.name)\n",
    "                for f in target_schema\n",
    "                if f.name in raw_df.columns  # Only cast columns that exist in raw data\n",
    "            ]\n",
    "            \n",
    "            # Apply casting and add audit timestamp for tracking when data was loaded\n",
    "            casted_df = raw_df.select(casted_cols).withColumn(\n",
    "                \"bronze_load_ts\", \n",
    "                F.current_timestamp()\n",
    "            )\n",
    "\n",
    "            # Step 8: Write the processed data to the bronze layer\n",
    "            casted_table = f\"your_catalog.bronze.bronze_{key}\"\n",
    "            \n",
    "            # Option 1: Overwrite existing data (commented out)\n",
    "            # Use this for full refresh scenarios\n",
    "            #casted_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(casted_table)\n",
    "            \n",
    "            # Option 2: Append mode with schema merging (active)\n",
    "            # Use this for incremental loads where schema may evolve\n",
    "            casted_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(casted_table)\n",
    "\n",
    "            print(f\"Successfully wrote casted table for {key}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Error handling: Log the error and continue with next table\n",
    "            # This prevents one failing table from stopping the entire pipeline\n",
    "            print(f\"Error writing tables for {key}: {str(e)}\")\n",
    "            df.printSchema()  # Print schema for debugging purposes\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown-3",
   "metadata": {},
   "source": [
    "## Execute the Pipeline\n",
    "\n",
    "Run the staging layer creation process for all configured tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_staging_layer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
